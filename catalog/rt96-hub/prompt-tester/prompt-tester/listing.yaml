description: Test and compare prompts across multiple LLM providers,
  specifically OpenAI and Anthropic. Manage multi-turn conversations, configure
  various parameters, and evaluate responses to optimize prompt strategies.
skills:
  - Test prompts with OpenAI models
  - Test prompts with Anthropic models
  - Configure system and user prompts
  - Return formatted responses or error messages
  - Track performance of prompts
  - Support for multi-turn conversations
